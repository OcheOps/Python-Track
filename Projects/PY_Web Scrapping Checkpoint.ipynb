{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724f253e",
   "metadata": {},
   "source": [
    "# Objective\n",
    "Scraping text from a Wikipedia website using Beautiful Soup\n",
    "\n",
    "**Instructions**\n",
    "After watching this video below, you will be able to:\n",
    "\n",
    "[Scraping Wikipedia with Python and Beautiful Soup](https://www.youtube.com/watch?v=YY5skv756pc)\n",
    "\n",
    "1. **Write a function to get and parse HTML content from a Wikipedia page.**\n",
    "\n",
    "2. **Write a function to extract the article title.**\n",
    "\n",
    "3. **Write a function to extract article text for each paragraph with their respective headings. Map those headings to their respective paragraphs in a dictionary.**\n",
    "\n",
    "4. **Write a function to collect every link that redirects to another Wikipedia page.**\n",
    "\n",
    "5. **Wrap all the previous functions into a single function that takes a Wikipedia link as a parameter.**\n",
    "\n",
    "6. **Test the final function on a Wikipedia page of your choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fc23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e1214",
   "metadata": {},
   "source": [
    "### 1. Write a function to Get and parse HTML content from a Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c395b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_content(url):\n",
    "    response = requests.get(url)  # Send an HTTP GET request to the given URL\n",
    "    if response.status_code == 200:  # Check if the request was successful\n",
    "        return response.text  # Return the HTML content of the page\n",
    "    else:\n",
    "        return None  # Return None if the request was not successful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfe57b",
   "metadata": {},
   "source": [
    "### 2. Write a function to Extract article title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')  # Parse the HTML content\n",
    "    title = soup.find('h1', id='firstHeading').text  # Find the title element and extract its text\n",
    "    return title  # Return the title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9130fd",
   "metadata": {},
   "source": [
    "### 3. Write a function to Extract article text for each paragraph with their respective headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs_with_headings(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')  # Parse the HTML content\n",
    "    content = soup.find('div', class_='mw-parser-output')  # Find the main content div\n",
    "    sections = {}  # Initialize an empty dictionary to store sections\n",
    "\n",
    "    current_heading = None  # Initialize the current heading as None\n",
    "    for element in content.find_all(['h2', 'p']):  # Loop through all headings and paragraphs\n",
    "        if element.name == 'h2':  # If the element is a heading\n",
    "            current_heading = element.text.strip()  # Set the current heading\n",
    "        elif element.name == 'p':  # If the element is a paragraph\n",
    "            if current_heading:  # If there is a current heading\n",
    "                if current_heading not in sections:\n",
    "                    sections[current_heading] = []  # Initialize a list for the heading if not present\n",
    "                sections[current_heading].append(element.text.strip())  # Append the paragraph to the heading\n",
    "    return sections  # Return the dictionary of sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb7765",
   "metadata": {},
   "source": [
    "### 4. Write a function to collect every link that redirects to another Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79428bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_links(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')  # Parse the HTML content\n",
    "    links = []  # Initialize an empty list to store links\n",
    "\n",
    "    for link in soup.find_all('a', href=True):  # Loop through all anchor tags with href attribute\n",
    "        href = link['href']\n",
    "        if href.startswith('/wiki/'):  # Check if the link redirects to another Wikipedia page\n",
    "            full_url = 'https://en.wikipedia.org' + href  # Construct the full URL\n",
    "            links.append(full_url)  # Add the full URL to the list\n",
    "    return links  # Return the list of links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460a117e",
   "metadata": {},
   "source": [
    "### 5. Wrap all the previous functions into a single function that takes a Wikipedia link as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7908ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wikipedia_page(url):\n",
    "    html_content = get_html_content(url)  # Get and parse HTML content\n",
    "    if html_content:\n",
    "        title = extract_title(html_content)  # Extract article title\n",
    "        sections = extract_paragraphs_with_headings(html_content)  # Extract paragraphs with headings\n",
    "        links = extract_wikipedia_links(html_content)  # Extract Wikipedia links\n",
    "        return {\n",
    "            'title': title,\n",
    "            'sections': sections,\n",
    "            'links': links\n",
    "        }  # Return a dictionary with the extracted information\n",
    "    else:\n",
    "        return None  # Return None if HTML content could not be retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99404ffa",
   "metadata": {},
   "source": [
    "### 6. Test the last function on a Wikipedia page of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Web_scraping'  # Specify the Wikipedia page URL\n",
    "result = process_wikipedia_page(url)  # Process the Wikipedia page\n",
    "\n",
    "# Print the extracted information\n",
    "if result:\n",
    "    print(\"Title:\", result['title'])\n",
    "    print(\"\\nSections:\")\n",
    "    for heading, paragraphs in result['sections'].items():\n",
    "        print(heading)\n",
    "        for paragraph in paragraphs:\n",
    "            print(paragraph)\n",
    "        print()\n",
    "    print(\"\\nLinks:\")\n",
    "    for link in result['links']:\n",
    "        print(link)\n",
    "else:\n",
    "    print(\"Failed to retrieve content from the Wikipedia page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad2b2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
